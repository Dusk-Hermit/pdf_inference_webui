[
    {
        "gpt_weight": "D:\\repos\\GPT-SoVITS-beta0306fix2\\GPT_weights\\auto_花火2_huahuo-e100.ckpt",
        "sovits_weight": "D:\\repos\\GPT-SoVITS-beta0306fix2\\SoVITS_weights\\auto_花火2_huahuo_e100_s1100.pth",
        "lines": [
            {
<<<<<<< HEAD
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要,\n我随时都可以帮你哦?谁能拒绝一位在鸡翅膀上打钉饰的男孩呢?",
                "prompt_language_text": "中文",
                "text": "and Yu et al .  2022; Müller et al  2022],\nbut struggle to achieve the visual quality obtained by the current SOTA NeRF methods,\ni.e.,\nMip-NeRF360 [Barron et al  2022],\nwhich requires up to 48 hours of training time. The fast – but lower-quality – radiance field methods can achieve interactive rendering times depending on the scene (10-15 frames per second),\nbut fall short of real-time rendering at high resolution. Our solution builds on three main components. We first intro- duce  as a flexible and expressive scene representation. We start with the same input as previous NeRF-like methods,\ni.e.,\ncameras calibrated with Structure-from-Motion (SfM) [Snavely et al 2006] and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process. In contrast to most point-based solutions that require Multi-View Stereo (MVS) data [Aliev et al  2020; Kopanas et al  2021; Rückert et al  2022],\nwe achieve high-quality results with only SfM points as input. Note that for the NeRF-synthetic dataset,\nour method achieves high qual- ity even with random initialization. We show that 3D Gaussians are an excellent choice,\nsince they are a differentiable volumetric representation,\nbut they can also be rasterized very efficiently by projecting them to 2D,\nand applying standard -blending,\nusing an equivalent image formation model as NeRF. The second component of our method is optimization of the properties of the 3D Gaussians – 3D position,\nopacity,\nanisotropic covariance,\nand spherical har- monic (SH) coefficients – interleaved with adaptive density control steps,\nwhere we add and occasionally remove 3D Gaussians during optimization. The optimization procedure produces a reasonably compact,\nunstructured,\nand precise representation of the scene (1-5 million Gaussians for all scenes tested). The third and final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization,\nfol- lowing recent work [Lassner and Zollhofer 2021]. However,\nthanks to our 3D Gaussian representation,\nwe can perform anisotropic splatting that respects visibility ordering – thanks to sorting and - blending – and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required. To summarize,\nwe provide the following contributions:",
                "text_language_text": "英文",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0002_block_0001.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要,\n我随时都可以帮你哦?谁能拒绝一位在鸡翅膀上打钉饰的男孩呢?",
                "prompt_language_text": "中文",
                "text": "Our results on previously published datasets show that we can opti- mize our 3D Gaussians from multi-view captures and achieve equal or better quality than the best quality previous implicit radiance field approaches. We also can achieve training speeds and quality similar to the fastest methods and importantly provide the first  with high quality for novel-view synthesis.",
                "text_language_text": "英文",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0002_block_0005.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要,\n我随时都可以帮你哦?谁能拒绝一位在鸡翅膀上打钉饰的男孩呢?",
                "prompt_language_text": "中文",
                "text": "similarity; radiance fields are a vast area,\nso we focus only on directly related work. For complete coverage of the field,\nplease see the excellent recent surveys [Tewari et al. 2022; Xie et al. 2022].",
                "text_language_text": "英文",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0002_block_0008.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要,\n我随时都可以帮你哦?谁能拒绝一位在鸡翅膀上打钉饰的男孩呢?",
                "prompt_language_text": "中文",
                "text": "The first novel-view synthesis approaches were based on light fields,\nfirst densely sampled [Gortler et al .  1996; Levoy and Hanrahan 1996] then allowing unstructured capture [Buehler et al  2001]. The advent of Structure-from-Motion (SfM) [Snavely et al  2006] enabled an entire new domain where a collection of photos could be used to synthesize novel views. SfM estimates a sparse point cloud during camera calibration,\nthat was initially used for simple visualization of 3D space. Subsequent multi-view stereo (MVS) produced im- pressive full 3D reconstruction algorithms over the years [Goesele et al  2007],\nenabling the development of several view synthesis algorithms [Chaurasia et al  2013; Eisemann et al  2008; Hedman  2018; Kopanas et al  2021]. All these methods  and  the input images into the novel view camera,\nand use the geometry to guide this re-projection. These methods produced ex- cellent results in many cases,\nbut typically cannot completely re- cover from unreconstructed regions,\nor from “over-reconstruction”,\nwhen MVS generates inexistent geometry. Recent neural render- ing algorithms [Tewari et al  2022] vastly reduce such artifacts and avoid the overwhelming cost of storing all input images on the GPU,\noutperforming these methods on most fronts.",
                "text_language_text": "英文",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0002_block_0010.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要,\n我随时都可以帮你哦?谁能拒绝一位在鸡翅膀上打钉饰的男孩呢?",
                "prompt_language_text": "中文",
                "text": "Deep learning techniques were adopted early for novel-view synthe- sis [Flynn et al .  2016; Zhou et al  2016]; CNNs were used to estimate blending weights [Hedman et al  2018],\nor for texture-space solutions [Riegler and Koltun 2020; Thies et al  2019]. The use of MVS-based geometry is a major drawback of most of these methods; in addition,\nthe use of CNNs for final rendering frequently results in temporal flickering. Volumetric representations for novel-view synthesis were ini- tiated by Soft3D [Penner and Zhang 2017]; deep-learning tech- niques coupled with volumetric ray-marching were subsequently proposed [Henzler et al  2019; Sitzmann et al  2019] building on a con- tinuous differentiable density field to represent geometry. Rendering using volumetric ray-marching has a significant cost due to the large number of samples required to query the volume. Neural Radiance Fields (NeRFs) [Mildenhall et al  2020] introduced importance sam- pling and positional encoding to improve quality,\nbut used a large Multi-Layer Perceptron negatively affecting speed. The success of NeRF has resulted in an explosion of follow-up methods that address quality and speed,\noften by introducing regularization strategies; the current state-of-the-art in image quality for novel-view synthesis is Mip-NeRF360 [Barron et al  2022]. While the rendering quality is outstanding,\ntraining and rendering times remain extremely high; we are able to equal or in some cases surpass this quality while providing fast training and real-time rendering. The most recent methods have focused on faster training and/or rendering mostly by exploiting three design choices: the use of spa- tial data structures to store (neural) features that are subsequently interpolated during volumetric ray-marching,\ndifferent encodings",
                "text_language_text": "英文",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0002_block_0012.wav"
=======
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "A central task in the application of probabilistic models is the evaluation of the posterior distribution p of the latent variables given the observed (visible) data variables.\nand the evaluation of expectations computed with respect to this distribution.\nThe model might also contain some deterministic parameters.\nwhich we will leave implicit for the moment.or it may be a fully Bayesian model in which any unknown parameters are given prior distributions and are absorbed into the set of latent variables denoted by the vector.\nFor instance.in the EM algorithm we need to evaluate the expectation of the complete-data log likelihood with respect to the posterior distribution of the latent variables.\nFor many models of practical interest.it will be infeasible to evaluate the posterior distribution or indeed to compute expectations with respect to this distribution.\nThis could be because the dimensionality of the latent space is too high to work with directly or because the posterior distribution has a highly complex form for which expectations are not analytically tractable.In the case of continuous variables.the required integrations may not have closed-form.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0481_block_0002.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "analytical solutions.while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration.\nFor discrete variables.the marginalizations involve summing over all possible configurations of the hidden variables.\nand though this is always possible in principle.we often find in practice that there may be exponentially many hidden states so that exact calculation is prohibitively expensive.\nIn such situations.we need to resort to approximation schemes.\nand these fall broadly into two classes.according to whether they rely on stochastic or deterministic approximations.\nStochastic techniques such as Markov chain Monte Carlo.\ndescribed in Chapter 11.have enabled the widespread use of Bayesian methods across many domains.\nThey generally have the property that given infinite computational resource.\nthey can generate exact results.and the approximation arises from the use of a finite amount of processor time.\nIn practice.sampling methods can be computationally demanding.\noften limiting their use to small-scale problems.Also.\nit can be difficult to know whether a sampling scheme is generating independent samples from the required distribution.\nIn this chapter.we introduce a range of deterministic approximation schemes.\nsome of which scale well to large applications.These are based on analytical approximations to the posterior distribution.\nfor example by assuming that it factorizes in a particular way or that it has a specific parametric form such as a Gaussian.\nAs such.they can never generate exact results.and so their strengths and weaknesses are complementary to those of sampling methods.\nIn Section 4.4.we discussed the Laplace approximation.\nwhich is based on a local Gaussian approximation to a mode (i.e..\na maximum) of the distribution.Here we turn to a family of approximation techniques called variational inference or variational Bayes.\nwhich use more global criteria and which have been widely applied.We conclude with a brief introduction to an alternative variational framework known as expectation propagation ..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0482_block_0001.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "Variational methods have their origins in the 18  century with the work of Euler.\nLagrange.and others on the calculus of variations.\nStandard calculus is concerned with finding derivatives of functions.\nWe can think of a function as a mapping that takes the value of a variable as the input and returns the value of the function as the output.\nThe derivative of the function then describes how the output value varies as we make infinitesimal changes to the input value.\nSimilarly.we can define a functional as a mapping that takes a function as the input and that returns the value of the functional as the output.An example would be the entropy p.which takes a probability distribution p x as the input and returns the quantity.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0482_block_0003.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "as the output.We can the introduce the concept of a functional derivative.\nwhich expresses how the value of the functional changes in response to infinitesimal changes to the input function (Feynman et al.\n1964).The rules for the calculus of variations mirror those of standard calculus and are discussed in Appendix D.\nMany problems can be expressed in terms of an optimization problem in which the quantity being optimized is a functional.\nThe solution is obtained by exploring all possible input functions to find the one that maximizes.\nor minimizes.the functional.Variational methods have broad applicability and include such areas as finite element methods (Kapur.\n1989) and maximum entropy (Schwarz.1988).Although there is nothing intrinsically approximate about variational methods.\nthey do naturally lend themselves to finding approximate solutions.\nThis is done by restricting the range of functions over which the optimization is performed.\nfor instance by considering only quadratic functions or by considering functions composed of a linear combination of fixed basis functions in which only the coefficients of the linear combination can vary.\nIn the case of applications to probabilistic inference.\nthe restriction may for example take the form of factorization assumptions (Jordan.\n1999; Jaakkola.2001).Now let us consider in more detail how the concept of variational optimization can be applied to the inference problem.\nSuppose we have a fully Bayesian model in which all parameters are given prior distributions.\nThe model may also have latent variables as well as parameters.\nand we shall denote the set of all latent variables and parameters by.\nSimilarly.we denote the set of all observed variables by.\nFor example.we might have a set of N independent.identically distributed data.\nfor which.and.Our probabilistic model specifies the joint distribution p.\nand our goal is to find an approximation for the posterior distribution as well as for the model evidence.As in our discussion of EM.we can decompose the log marginal probability using.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0483_block_0001.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "This differs from our discussion of EM only in that the parameter vector no longer appears.\nbecause the parameters are now stochastic variables and are absorbed into.\nSince in this chapter we will mainly be interested in continuous variables we have used integrations rather than summations in formulating this decomposition.\nHowever.the analysis goes through unchanged if some or all of the variables are discrete simply by replacing the integrations with summations as required.\nAs before.we can maximize the lower bound q by optimization with respect to the distribution.\nwhich is equivalent to minimizing the KL divergence.\nIf we allow any possible choice for q.then the maximum of the lower bound occurs when the KL divergence vanishes.which occurs when equals the posterior distribution p ..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0483_block_0010.wav"
>>>>>>> d5f10ec6d67e3b94056fbb2edb9638e830755485
            }
        ]
    }
]