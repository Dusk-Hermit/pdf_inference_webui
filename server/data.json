[
    {
        "gpt_weight": "D:\\repos\\GPT-SoVITS-beta0306fix2\\GPT_weights\\auto_花火2_huahuo-e100.ckpt",
        "sovits_weight": "D:\\repos\\GPT-SoVITS-beta0306fix2\\SoVITS_weights\\auto_花火2_huahuo_e100_s1100.pth",
        "lines": [
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要.我随时都可以帮你哦.谁能拒绝一位在鸡翅膀上打钉饰的男孩呢.",
                "prompt_language_text": "中文",
                "text": "Fig.1.Our method achieves real-time rendering of radiance fields with quality that equals the previous method with the best quality [Barron et al.\n2022].while only requiring optimization times competitive with the fastest previous methods [Fridovich-Keil and Yu et al 2022; Müller et al 2022].\nKey to this performance is a novel 3D Gaussian scene representation coupled with a real-time differentiable renderer.\nwhich offers significant speedup to both scene optimization and novel view synthesis.\nNote that for comparable training times to InstantNGP [Müller et al 2022].\nwe achieve similar quality to theirs; while this is the maximum quality they reach.\nby training for 51min we achieve state-of-the-art quality.even slightly better than Mip-NeRF360 [Barron et al 2022]..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0001_block_0003.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要.我随时都可以帮你哦.谁能拒绝一位在鸡翅膀上打钉饰的男孩呢.",
                "prompt_language_text": "中文",
                "text": "Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.\nHowever.achieving high visual quality still requires neural networks that are costly to train and render.\nwhile recent faster methods inevitably trade off speed for quality.\nFor unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering.\nno current method can achieve real-time display rates.\nWe introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ( 30 fps) novel-view synthesis at 1080p resolution.\nFirst.starting from sparse points produced during camera calibration.\nwe represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second.\nwe perform interleaved optimization/density control of the 3D Gaussians.\nnotably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third.\nwe develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering.We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0001_block_0004.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要.我随时都可以帮你哦.谁能拒绝一位在鸡翅膀上打钉饰的男孩呢.",
                "prompt_language_text": "中文",
                "text": "Authors’ addresses: Bernhard Kerbl.bernhard.kerbl@inria.fr.\nInria.Université Côte d’Azur.France; Georgios Kopanas.\ngeorgios.kopanas@inria.fr.Inria.Université Côte d’Azur.\nFrance; Thomas Leimkühler.thomas.leimkuehler@mpi-inf.mpg.de.\nMaxPlanck-Institut für Informatik.Germany; George Drettakis.\ngeorge.drettakis@inria.fr.Inria.Université Côte d’Azur.France..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0001_block_0007.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要.我随时都可以帮你哦.谁能拒绝一位在鸡翅膀上打钉饰的男孩呢.",
                "prompt_language_text": "中文",
                "text": "Publication rights licensed to ACM.ACM acknowledges that this contribution was authored or co-authored by an employee.\ncontractor or affiliate of a national government.As such.\nthe Government retains a nonexclusive.royalty-free right to publish or reproduce this article.\nor to allow others to do so.for Government purposes only.\n© 2023 Copyright held by the owner/author(s).Publication rights licensed to ACM.0730-0301/2023/8-ART1 $15.00 https://doi.org/10.1145/3592433.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0001_block_0008.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要.我随时都可以帮你哦.谁能拒绝一位在鸡翅膀上打钉饰的男孩呢.",
                "prompt_language_text": "中文",
                "text": "Bernhard Kerbl.Georgios Kopanas.Thomas Leimkühler.\nand George Drettakis.2023.3D Gaussian Splatting for Real-Time Radiance Field Rendering.42.4.Article 1 (August 2023).14 pages.https: //doi.org/10.1145/3592433.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0001_block_0010.wav"
            },
            {
                "ref_wav_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\temp_ref.wav",
                "prompt_text": "好吧好吧～我只是想说…如果你需要.我随时都可以帮你哦.谁能拒绝一位在鸡翅膀上打钉饰的男孩呢.",
                "prompt_language_text": "中文",
                "text": "Meshes and points are the most common 3D scene representations because they are explicit and are a good fit for fast GPU/CUDA-based rasterization.\nIn contrast.recent Neural Radiance Field (NeRF) methods build on continuous scene representations.\ntypically optimizing a Multi-Layer Perceptron (MLP) using volumetric ray-marching for novel-view synthesis of captured scenes.\nSimilarly.the most efficient radiance field solutions to date build on continuous representations by interpolating values stored in.\ne.g..voxel [Fridovich-Keil and Yu et al.2022] or hash [Müller et al 2022] grids or points [Xu et al 2022].\nWhile the continuous nature of these methods helps optimization.\nthe stochastic sampling required for rendering is costly and can result in noise.\nWe introduce a new approach that combines the best of both worlds: our 3D Gaussian representation allows optimization with state-of-the-art (SOTA) visual quality and competitive training times.\nwhile our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously published datasets [Barron et al 2022; Hedman et al 2018; Knapitsch et al.\n2017] (see Fig.1).Our goal is to allow real-time rendering for scenes captured with multiple photos.\nand create the representations with optimization times as fast as the most efficient previous methods for typical real scenes.Recent methods achieve fast training [Fridovich-Keil.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\repos\\pdf_voice_inference_webui\\output\\2308.04079v1\\voice_output\\page_0001_block_0012.wav"
            }
        ]
    }
]