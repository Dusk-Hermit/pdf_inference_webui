[
    {
        "gpt_weight": "D:\\git_download\\GPT-SoVITS-beta0217\\GPT_weights\\saileach-e15.ckpt",
        "sovits_weight": "D:\\git_download\\GPT-SoVITS-beta0217\\SoVITS_weights\\saileach_e8_s232.pth",
        "lines": [
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "A central task in the application of probabilistic models is the evaluation of the posterior distribution p of the latent variables given the observed (visible) data variables.\nand the evaluation of expectations computed with respect to this distribution.\nThe model might also contain some deterministic parameters.\nwhich we will leave implicit for the moment.or it may be a fully Bayesian model in which any unknown parameters are given prior distributions and are absorbed into the set of latent variables denoted by the vector.\nFor instance.in the EM algorithm we need to evaluate the expectation of the complete-data log likelihood with respect to the posterior distribution of the latent variables.\nFor many models of practical interest.it will be infeasible to evaluate the posterior distribution or indeed to compute expectations with respect to this distribution.\nThis could be because the dimensionality of the latent space is too high to work with directly or because the posterior distribution has a highly complex form for which expectations are not analytically tractable.In the case of continuous variables.the required integrations may not have closed-form.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0481_block_0002.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "analytical solutions.while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration.\nFor discrete variables.the marginalizations involve summing over all possible configurations of the hidden variables.\nand though this is always possible in principle.we often find in practice that there may be exponentially many hidden states so that exact calculation is prohibitively expensive.\nIn such situations.we need to resort to approximation schemes.\nand these fall broadly into two classes.according to whether they rely on stochastic or deterministic approximations.\nStochastic techniques such as Markov chain Monte Carlo.\ndescribed in Chapter 11.have enabled the widespread use of Bayesian methods across many domains.\nThey generally have the property that given infinite computational resource.\nthey can generate exact results.and the approximation arises from the use of a finite amount of processor time.\nIn practice.sampling methods can be computationally demanding.\noften limiting their use to small-scale problems.Also.\nit can be difficult to know whether a sampling scheme is generating independent samples from the required distribution.\nIn this chapter.we introduce a range of deterministic approximation schemes.\nsome of which scale well to large applications.These are based on analytical approximations to the posterior distribution.\nfor example by assuming that it factorizes in a particular way or that it has a specific parametric form such as a Gaussian.\nAs such.they can never generate exact results.and so their strengths and weaknesses are complementary to those of sampling methods.\nIn Section 4.4.we discussed the Laplace approximation.\nwhich is based on a local Gaussian approximation to a mode (i.e..\na maximum) of the distribution.Here we turn to a family of approximation techniques called variational inference or variational Bayes.\nwhich use more global criteria and which have been widely applied.We conclude with a brief introduction to an alternative variational framework known as expectation propagation ..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0482_block_0001.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "Variational methods have their origins in the 18  century with the work of Euler.\nLagrange.and others on the calculus of variations.\nStandard calculus is concerned with finding derivatives of functions.\nWe can think of a function as a mapping that takes the value of a variable as the input and returns the value of the function as the output.\nThe derivative of the function then describes how the output value varies as we make infinitesimal changes to the input value.\nSimilarly.we can define a functional as a mapping that takes a function as the input and that returns the value of the functional as the output.An example would be the entropy p.which takes a probability distribution p x as the input and returns the quantity.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0482_block_0003.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "as the output.We can the introduce the concept of a functional derivative.\nwhich expresses how the value of the functional changes in response to infinitesimal changes to the input function (Feynman et al.\n1964).The rules for the calculus of variations mirror those of standard calculus and are discussed in Appendix D.\nMany problems can be expressed in terms of an optimization problem in which the quantity being optimized is a functional.\nThe solution is obtained by exploring all possible input functions to find the one that maximizes.\nor minimizes.the functional.Variational methods have broad applicability and include such areas as finite element methods (Kapur.\n1989) and maximum entropy (Schwarz.1988).Although there is nothing intrinsically approximate about variational methods.\nthey do naturally lend themselves to finding approximate solutions.\nThis is done by restricting the range of functions over which the optimization is performed.\nfor instance by considering only quadratic functions or by considering functions composed of a linear combination of fixed basis functions in which only the coefficients of the linear combination can vary.\nIn the case of applications to probabilistic inference.\nthe restriction may for example take the form of factorization assumptions (Jordan.\n1999; Jaakkola.2001).Now let us consider in more detail how the concept of variational optimization can be applied to the inference problem.\nSuppose we have a fully Bayesian model in which all parameters are given prior distributions.\nThe model may also have latent variables as well as parameters.\nand we shall denote the set of all latent variables and parameters by.\nSimilarly.we denote the set of all observed variables by.\nFor example.we might have a set of N independent.identically distributed data.\nfor which.and.Our probabilistic model specifies the joint distribution p.\nand our goal is to find an approximation for the posterior distribution as well as for the model evidence.As in our discussion of EM.we can decompose the log marginal probability using.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0483_block_0001.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\temp_ref.wav",
                "prompt_text": "您是不是累了.这是我烤的甜司康饼.您可以坐下来.喝点茶.与我说说您的烦心事.",
                "prompt_language_text": "中文",
                "text": "This differs from our discussion of EM only in that the parameter vector no longer appears.\nbecause the parameters are now stochastic variables and are absorbed into.\nSince in this chapter we will mainly be interested in continuous variables we have used integrations rather than summations in formulating this decomposition.\nHowever.the analysis goes through unchanged if some or all of the variables are discrete simply by replacing the integrations with summations as required.\nAs before.we can maximize the lower bound q by optimization with respect to the distribution.\nwhich is equivalent to minimizing the KL divergence.\nIf we allow any possible choice for q.then the maximum of the lower bound occurs when the KL divergence vanishes.which occurs when equals the posterior distribution p ..",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\Pattern Recognition and Machine Learning (Information Science and Statistics)\\voice_output\\page_0483_block_0010.wav"
            }
        ]
    }
]