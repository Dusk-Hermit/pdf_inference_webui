[
    {
        "gpt_weight": "D:\\git_download\\GPT-SoVITS-beta0217\\GPT_weights\\saileach-e15.ckpt",
        "sovits_weight": "D:\\git_download\\GPT-SoVITS-beta0217\\SoVITS_weights\\saileach_e8_s232.pth",
        "lines": [
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\temp_ref.wav",
                "prompt_text": "您是不是累了?这是我烤的甜司康饼,\n您可以坐下来,\n喝点茶,\n与我说说您的烦心事",
                "prompt_language_text": "中文",
                "text": "Recurrent neural networks,\nlong short-term memory   and gated recurrent   neural networks in particular,\nhave been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation  . Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures  .",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\voice_output\\page_0002_block_0001.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\temp_ref.wav",
                "prompt_text": "您是不是累了?这是我烤的甜司康饼,\n您可以坐下来,\n喝点茶,\n与我说说您的烦心事",
                "prompt_language_text": "中文",
                "text": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time,\nthey generate a sequence of hidden states,\nas a function of the previous hidden state  and the input for position . This inherently sequential nature precludes parallelization within training examples,\nwhich becomes critical at longer sequence lengths,\nas memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks   and conditional computation,\nwhile also improving model performance in case of the latter. The fundamental constraint of sequential computation,\nhowever,\nremains.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\voice_output\\page_0002_block_0002.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\temp_ref.wav",
                "prompt_text": "您是不是累了?这是我烤的甜司康饼,\n您可以坐下来,\n喝点茶,\n与我说说您的烦心事",
                "prompt_language_text": "中文",
                "text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU,\nByteNet   and ConvS2S,\nall of which use convolutional neural networks as basic building block,\ncomputing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions,\nlinearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions  . In the Transformer this is reduced to a constant number of operations,\nalbeit at the cost of reduced effective resolution due to averaging attention-weighted positions,\nan effect we counteract with Multi-Head Attention as described in section 3.2.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\voice_output\\page_0002_block_0006.wav"
            },
            {
                "ref_wav_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\temp_ref.wav",
                "prompt_text": "您是不是累了?这是我烤的甜司康饼,\n您可以坐下来,\n喝点茶,\n与我说说您的烦心事",
                "prompt_language_text": "中文",
                "text": "Most competitive neural sequence transduction models have an encoder-decoder structure  . Here,\nthe encoder maps an input sequence of symbol representations  to a sequence of continuous representations . Given,\nthe decoder then generates an output sequence  of symbols one element at a time. At each step the model is auto-regressive,\nconsuming the previously generated symbols as additional input when generating the next.",
                "text_language_text": "中英混合",
                "output_file_path": "D:\\git_download\\GPT-SoVITS-beta0217\\pdf_inference_webui\\output\\1706.03762\\voice_output\\page_0002_block_0011.wav"
            }
        ]
    }
]